## 分布式服务

#### 集中式服务
由一台或多台计算机组成中心节点，数据集中存储在中心节点中，并且整个系统所有业务单元都集中在这个中心节点上，所有的功能均由其集中处理

优点：
* 结构简单
* 部署简单
* 项目架构简单

缺点：
* 大型主机的研发人才和维护人才培养成本高
* 大型主机非常昂贵
* 单点故障问题
* 大型主机的性能扩展受限于摩尔定律

服务扩展方式：
* 纵向扩展：提升服务器性能，理论上是受限的
* 横向扩展：提升服务器数量，理论上是不受限的


#### 分布式服务
一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。

特点：
* 分布性：分布式系统中的多台计算机都会在空间上随意分布，同时，它们的分布情况也会随时变化
* 对等性：集群中的每个工作节点的角色是一样的
* 并发性：多个机器可能同时操作一个数据库或存储系统
* 缺乏全局时钟：分布式系统中的多个主机上的事件的先后顺序很难界定
* 故障总发生：服务器宕机、网络阻塞或延迟

传统集群和分布式的区别：
* 传统集群：大量机器组成团队，大家做同一件事，只是这些事的数量比较大
* 分布式：大量集群组成团队，每个成员完成其中一部分，大家做的是不同的事情

分布式的异常问题：
* 通信异常：网络不可用，消息延迟或丢失，会导致分布式系统内部无法顺利进行一次网络通信，可能造成多节点数据丢失和状态不一致，还可能造成数据乱序
* 网络分区：网络不联通，但各个子系统的内部网络是正常的，从而导致整个系统被切分成了若干个孤立的区域，会出现局部小集群造成数据不一致
* 节点故障/机器宕机
* 分布式三态：即成功、失败和超时
* 存储数据丢失：对于有状态节点，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储的状态。通过副本来解决
* 异常处理原则：除非需求指标允许，否则在系统设计时不能放过任何异常情况

衡量分布式系统的性能指标：
* 性能，三个性能指标相互制约，追求高吞吐量，那么延迟不会低；响应时间较长时，很难提高QPS
    * 系统的吞吐能力，指某一时间段内可以处理的数据总量，通常可以用系统每秒处理的总数据量来衡量
    * 系统的响应延迟，指系统完成某一功能需要使用的时间
    * 系统的并发能力，指系统同时完成某一功能的能力，通常用QPS 来衡量
* 可用性，可使用系统停服时间与正常服务时间的比例来衡量
* 可扩展性
* 一致性

#### 一致性的理解
* 强一致性：写操作完成之后，读操作一定能读到最新数据。很难实现，Paxos 算法、Quorum 机制、ZAB 协议等可以实现。
* 弱一致性：不承诺立即可以读到最新的数据，会尽可能保证到某个时间点后，数据能够达到一致
* 读写一致性：用户读取自己写入结果的一致性
    * 一些特定内容从主库读取
    * 设置一个更新时间窗口，更新期都从主库读取，之后从最新的从库读取
    * 记录数据时间戳，在请求时把用户的时间戳带上，更新时间小于时间戳的从库都不响应请求
* 单调一致性：本次读到的数据不能比上次读到的旧
    * 可能出现的问题：多次刷新返回旧数据，通过hash 映射到同一台机器上来解决
* 因果一致性：如果节点A 在更新完某个数据后通知了节点B，那么节点B 后续对该数据的访问和修改都是基于A 更新后的值；与A 无关系的节点C的数据访问则没有这样的限制
* 最终一致性：所有分布式一致性模型中最弱的，不考虑中间的任何状态，只保证经过一段时间之后，最终系统内数据一致。

分布式一致性的作用：
* 为了提高系统的可用性，以防止单点故障引起的系统不可用
* 提高系统的整体性能，通过负载技术，能够让分布在不同地方的副本，都能够为用户服务


## 分布式事务

事务，是用来保证存储系统的数据状态的一致性的
* 广义上的事务：一个事务中的所有操作，要么都成功，要么都失败，没有中间状态
* 狭义上的事务：数据库的事务

事务的特征：ACID：原子性、一致性、持久性、隔离性

#### 分布式事务提交模式

##### 2PC 两阶段提交

第一阶段：请求/表决阶段
* 在分布式事务发起者向分布式事务协调者发送请求时，事务协调者向所有参与者发送事务预处理请求 vote request
* 各个参与者会开启本地事务并开始执行本地事务，执行完成后不会commit，而是向事务协调者报告是否可以处理本次事务

第二阶段：提交/执行/回滚阶段
* 事务协调者收到所有参与者反馈后，若所有参与者均可以提交，则通知参与者和发起者执行commit，否则rollback

![](https://oscimg.oschina.net/oscnet/up-7e70cbd3a6b9169210183f6bf04c57b9b5a.png)

2PC 的问题
* 性能问题，同步阻塞：各个节点都处于阻塞状态，只有当所有节点准备完毕，协调者才会通知进行全局commit/rollback，参与者才会释放锁定的资源
* 单点故障问题，协调者可能宕机
* 数据不一致，消息丢失问题：如果发生网络问题，一部分参与者接收不到commit/rollback 消息，那么就会导致节点间数据不一致
* 没有容错机制：必须所有的参与者都正反馈才提交事务，如果有一个事参与者的响应没有收到，整个事务就会失败

##### 3PC 三阶段提交
将二阶段提交的"提交事务请求"一分为二，变为 CanCommit、PreCommit、DoCommit 三个阶段，并且还引入了超时机制，参与者超时未收到协调者信息，则本地自动commit

第一阶段：CanCommit 提交询问阶段
* 协调者询问参与者释放可以进行事务操作，参与者根据实际情况进行反馈

第二阶段：PreCommit 预提交阶段
* 若参与者返回的都是同意，协调者则向参与者发送预提交请求，并进入Prepared 阶段
* 参与者收到预提交请求后，执行事务操作，并保存Undo 和Redo 到事务日志中
* 参与者执行完本地事务后（uncommitted），会向协调者发送Ack 表示已准备好提交事务
* 如果协调者收到预提交响应为拒绝或超时，则执行中断事务操作，通知参与者中断事务
* 参与者收到中断事务或超时，都会主动中断事务或直接提交

第三阶段：DoCommit 最终提交阶段
* 协调者收到所有的Ack，则从预提交进入提交阶段，并向参与者发送提交请求
* 参与者收到提交请求，正式提交事务commit，并反馈给协调者结果

3PC 的问题：
* 降低阻塞范围：询问以及超时机制，都降低了阻塞时间和范围，而且超时后，自动提交的机制也可以释放资源
* 依然可能数据不一致：在进入PreCommit 阶段后，网络出现问题，则会自动提交，有可能会出现数据不一致的情况


## 分布式一致性算法

分布式系统中的节点通信存在两种模型：共享内存和消息传递

### Paxos 算法

是一种基于消息传递且具有高度容错特性的一致性算法。

核心思想是少数服从多数，来源于议会制

##### Paxos 中，存在三种角色：
* Proposer 提议者，用来发出提案 proposal
* Acceptor 接受者，可以接收或拒绝提案
* Learner 学习者，学习被批准的提案，当提案被超过半数的Acceptor 接受后为被批准

映射到zookeeper ：
* leader 发起提案  leader 故障时，会有leader 选举机制，仅有一个leader，保证提出提案的顺序
* follower 参与投票
* observer 被动接受提案

##### Paxos 要解决的问题
* 决议value 只有在被Proposer 提出后才能被批准
* 在一次Paxos 算法执行实例中，只批准chose 一个决议value
* Learner 只能获得被批准chose 的决议value

```
所有事务请求必须由一个全局的服务器来协调处理，这样的服务器称为leader 服务器，余下的服务器称为follower 服务器。
leader 服务器负载将一个客户端事务请求转换为一个事务proposal，并将该proposal 分发给集群中所有的follower。
之后leader 要等待所有follower 的反馈，一旦超过半数的follower 进行了正反馈，那么leader 会再次向follower 分发commit 消息，要求将前一个proposal 提交
```

##### Raft 算法
参考：http://thesecretlivesofdata.com/raft/

##### ZAB 协议
zookeeper 底层工作机制，ZAB 协议有两种基本的模式：崩溃恢复和消息广播

**ZAB 协议需要确保那些已经在leader 服务器上提交的事务最终被所有服务器都提交**
**ZAB 协议需要确保丢弃那些只在leader 服务器上被提出的事务**

如果让leader 选举算法能够保证新选举出来的leader 拥有集群中所有集群最高事务编号ZXID 的事务proposal，那么就可以保证这个新选举出来的leader 一定具有所有已提交的事务


## 抽屉原理/鸽巢原理

* 若有n个笼子和n+1个鸽子，所有的鸽子都被关在鸽笼里，那么至少有一个笼子有2只鸽子
* 若有n个笼子和kn+1个鸽子，所有的鸽子都被关在鸽笼里，那么至少有一个笼子里有k+1个鸽子

该原理变变型，总共n个节点有数据，一次更新数据的操作，我们不需要更新全部数据，就可以获得有效数据，当然需要我们读取多个副本来完成。

例如，更新了m个节点，m<n，那么我们想读取到最新的数据，至少我们读取n-m+1 次即可

## Quorum NWR 机制
在分布式场景中常用的，用来保证数据安全，并且在分布式环境中实现最终一致性的投票算法。基于鸽巢原理，能实现强一致性，而且还能自定义一致性级别

* N：复制的节点数，即一份数据被保存的副本数
* W：写操作成功的节点数，即每次数据写入写成功的副本数，W 肯定小于等于N
* R：读操作获取最新版本数据所需的最小节点数，即每次读取成功至少需要读取的副本数

**这三个因素决定了可用性、一致性和分区容错性，只有保证 W+R>N 就一定能读取到最新的数据，数据一致性级别完全可以根据读写副本数的约束来达到强一致性！**

#### 自定义分布式的性能级别
当N 固定时，分为三种情况：
* **W=1 R=N  write once read all**
    * 分布式环境中，写一份，如果要读取到最新的数据，就必须读取所有的节点
    * 写效率高，读效率低，一致性高，分区容错性差，可用性高
* **R=1 W=N  read only write all**
    * 分布式环境中，所有节点同步完成，才能读取，所有读取一个节点就可以读取到最新的数据
    * 读操作高效，写效率低，分区容错性好，一致性低，可用性高
* **W=Q R=Q where Q = N/2+1**
    * 写超过一半的节点，读超过一半的节点，取得读写性能平衡
    * 一半应用适用，读写性能平衡，分区容错性好，可用性、一致性取得一个平衡

*zookeeper 就采用了第三种情况*


## CAP 理论
一个分布式系统不可能同时满足CAP 三个需求
* **C：Consistency 强一致性**，分布式环境中多个数据副本一致
* **A：Availability 高可用性**，系统提供的服务必须一致处于可用，对于用户的每次操作总能在有限的时间内返回结果
* **P：Partition Tolerance 分区容错性**，遇到网络分区故障时，仍能够保证对外提供满足一致性和可用性的服务

CAP 只能三选二，因为在分布式系统中，容错性P 肯定是必须有的，那么遇到网络问题要么错误返回，要么阻塞等待，前者牺牲了一致性，后者牺牲了可用性。

## BASE 理论
核心思想：即时无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性
* **Basically Available 基本可用**
    * 允许在出现故障时，损失部分可用性，即保证核心可用
    * 响应时间的损失：出现故障或高峰时，适当延长响应时间
    * 功能上的损失：例如淘宝双11，下单功能可用，其他边缘功能暂时不可用
* **Soft State 软状态**
    * 允许系统存在中间状态，而该中间状态不影响系统整体的可用性
    * 即在不同节点同步数据时存在延迟，且出现数据同步延迟时存在的中间状态不影响系统的整体性能
* **Eventually Consistent 最终一致**
    * 系统中的所有数据副本经过一段时间后，最终达到一致的状态
    

## Zookeeper 核心架构和工作机制
Zookeeper 是一个分布式应用程序协调服务，它提供了简单原始的功能，分布式应用可以基于它实现更高级的服务，比如分布式同步、配置管理、集群管理、命名管理、队列管理。

Zookeeper 是集群的管理者，监视着集群中各节点的状态，根据节点提交的反馈进行下一步合理的操作，最终将简单易用的接口、功能稳定、性能高效的系统提供给用户。

Zookeeper 以一个集群的方式对外提供协调服务，集群内部的节点都保存了一份完整的数据。
* Zookeeper 中引入了leader、follower、observer 三种角色
* 通过leader 选举选定一台leader 节点，leader 角色为客户提供写服务，其他角色提供读服务
* observer 不参与leader 选举过程、写操作过半成功策略，因此可以在不影响写性能的前提下提高集群性能
* 集群中的所有节点的数据状态通过ZAB 协议保持一致

架构特点：
* 最终一致性：不论访问哪个节点，得到的都是同一个数据视图
* 可靠性
* 实时性：保证客户端将在一个时间间隔范围内获得服务端的更新信息，或者服务器失效的信息。若需要最新信息，则在读之前调用sync() 接口
* 等待无关：慢的或失效的请求不能干预快的请求
* 原子性：事务操作只有成功或失败，没有中间状态
* 顺序性：
    * TCP 协议保证消息的全序特性，先发的消息，服务器先收到
    * leader 的因果顺序，包括全局有序和偏序两种
        * 全局有序：如果一台服务器上的消息是a 早于b 发布，那么所有服务器上消息a 都将在b 之前发布
        * 偏序：如果消息b 在消息a 之后被同一个发布者发布，那么a 必将排在b 之前

Zookeeper 的主要组件
* znode 节点
* watch 监听
* ACL 权限控制系统


## zookeeper 安装

官网下载软件包，并解压到安装目录。安装分为单机模式、伪集群模式、集群模式

### 单机模式

* 修改配置文件，${zk_home}/conf/zoo.cfg，可以从zoo_sample.cfg 拷贝
```
# 单位毫秒，用来控制心跳间隔和超时时间，默认情况下最小的会话超时时间为两倍的tickTime
tickTime=2000
# 允许follower 连接并同步到leader 的初始化连接时间，以tickTime 的倍数来表示。当超过设置的时间，则连接失败
initLimit=10
# leader 与follower 之间发送消息、请求和应答的时间长度。如果在设置的时间内follower 不能与leader 进行通信，那么follower 将被丢弃
syncLimit=5
# 存储内存中数据快照的位置
dataDir=${zk_home}/data
# 日志的存储目录，使用专有的日志存储设备将大大提高系统的性能
dataLogDir=${zk_home}/log
# 监听客户端连接的端口
clientPort=2181
# 快照和日志最多保留的文件数量
autopurge.snapRetainCount=30
# 快照和日志自动清理的周期，单位：小时
autopurge.purgeInterval=24

# zookeeper 服务器，格式：server.id=主机名:心跳端口:选举端口
server.1=127.0.0.1:2888:3888
```

* 启动、关闭服务
```
# 启动
${zk_home}/bin/zkServer.sh start
# 关闭
${zk_home}/bin/zkServer.sh stop
# 查看节点状态
${zk_home}/bin/zkServer.sh status
```

### 伪集群模式

* 在一台机器启动多个zookeeper 服务，并组成集群，一般启动奇数个服务
* 先复制zookeeper 得到三个实例，${zk_home1} ${zk_home2} ${zk_home3}，然后修改zoo.cfg，之后分别启动三个服务
```
# zk1
clientPort=2181
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
server.3=127.0.0.1:2890:3890

# zk2
clientPort=2182
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
server.3=127.0.0.1:2890:3890

# zk3
clientPort=2183
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
server.3=127.0.0.1:2890:3890

# 若需要配置observer 角色
# server.4=127.0.0.1:2891:3891:observer
``` 
![](https://oscimg.oschina.net/oscnet/up-618f1a1bc33a401e861c1475cedece543ea.png)

### 集群模式

* 在多台机器上运行zookeeper 服务
* 配置zoo.cfg 文件，server 配置对应的机器地址即可


## zookeeper 的znode 存储系统

zookeeper 的文件系统是树形结构，每个节点叫做znode，既是文件夹也是文件，每个znode 有唯一的路径标识，既能存储数据，也能创建子znode。
但znode 只适合存储非常小的数据，不能超过1M，最好小于1K。

znode 的分类：
* 按照生命周期划分：
    * 短暂 ephemeral 断开连接自己删除，并且不能有子节点
    * 持久 persistent 断开连接不删除，默认情况
* 按照是否自带序列编号划分
    * sequential 带自增序列编号，由父节点维护
    * 非sequential 不带自增序列编号，默认情况


## zookeeper 的监听机制

客户端注册监听它关心的znode，当znode 发生变化（数据改变、节点删除、子目录节点增加删除）时，zookeeper 会通知客户端。

监听器的工作机制，其实时在客户端会专门创建一个监听线程，在本机的一个端口等待zookeeper 集群发送过来事件。

* 有三种注册监听的方式
* 有五种触发监听的方式
* 有四种类型的事件
![](https://oscimg.oschina.net/oscnet/up-eef9d9c00704108ca867a01cb07956b34fc.png)

监听工作原理：
* zookeeper 的watch 机制主要包括客户端线程、客户端WatchManager、zookeeper 服务器三部分
* 客户端向zookeeper 服务器注册监听的同时，会将watch 对象存储在客户端的WatchManager 中
* 当zookeeper 服务器触发watch 事件后，会向客户端发送通知WatchEvent，客户端线程从WatchManager 中取出对应的watch 对象来执行回调逻辑
    * WatchEvent 包含：节点路径 Path、事件类型 EventType、连接信息 connect


## zookeeper 应用场景

#### 发布/订阅

* 发布订阅有两种设计模式，推Push 和拉Pull
    * 推Push 模式：服务端将所有数据更新发给订阅的客户端
    * 拉Pull 模式：客户端主动发起请求获取最新数据，通常采用轮询
* zk 采用推拉结合，客户端向服务端注册自己需要关注的节点，一旦节点数据发生变化，服务器向客户端发送WatchEvent 通知，客户端收到通知主动向服务器获取最新数据。
* 这种模式主要用于配置信息的获取同步

#### 命名服务

* 命名服务：客户端可以根据指定名字来获取资源的实体、服务地址和提供者的信息。分布式场景中，被命名的实体通常是集群中的机器、提高服务的服务地址或远程对象等
* zk 可以帮助应用系统通过资源引用的方式实现对资源的定位和使用，在分布式环境中，上层应用仅仅需要一个全局唯一的名字来确定资源
* zk 可以实现一套分布式全局唯一ID 的分配机制，使用顺序节点，保证同一节点下的子节点是唯一的

#### 配置管理

系统配置可以放在zk 上的某个节点中，然后所有的应用对这个目录进行监听，一旦配置信息发生变化，每个应用程序都会收到zk 的通知，然后再从zk 获取配置信息

##### 集群管理

主要是两点：
* 是否有机器退出或加入
    * 所有机器约定在父目录下创建临时目录节点，然后监听父目录的子节点变化消息
    * 一旦有机器挂掉，该机器与zookeeper 连接断开，那么其创建的临时节点也会被删除，所有其他机器会收到通知
* 选举master
    * 所有机器创建临时顺序目录节点，每次选取编号最小的机器作为master 即可

##### 分布式锁

锁服务有三类：
* 写锁，对写加锁，保持独占，或者叫排它锁，独占锁
    * 在zookeeper 上，通过createznode() 的方式来实现
    * 所有客户端都去创建/lock 节点，最终创建成功的客户端即持有这把锁，用完之后删除/lock 节点即释放了这把锁
* 读锁，对读加锁，可共享访问，释放锁之后才可以进行事务操作，也叫共享锁
* 控制时序，叫时序锁
    * 通过创建临时顺序目录节点来实现
    * 客户端都创建临时顺序目录节点，并监听父节点的子节点变化
    * 编号最小的获得锁，执行，用完之后删除节点，其他客户端通过监听，获取变化，依次执行

##### 队列管理

队列有两种类型：
* 同步队列/分布式屏障：当队列的成员都聚齐时，队列才可用，否则一直等待所有成员到达
    * 在约定目录下创建临时目录节点，监听节点数是否达到要求的数量
* 先进先出队列：队列按照FIFO 方式进行入队和出队操作
    * 创建顺序节点，入列有编号，出列按编号

##### 负载均衡

轮询机制，借鉴时序锁的实现机制


## zookeeper 序列化和网络通讯协议

#### 序列化

序列化API 主要在zookeeper-jute 模块中
* org.apache.jute.InputArchive 反序列化需要实现的接口
    * 实现类有三种，org.apache.jute.BinaryInputArchive org.apache.jute.CsvInputArchive org.apache.jute.XmlInputArchive
* org.apache.jute.OutputArchive 序列化操作都需要实现这个接口
    * 实现类有三种，org.apache.jute.BinaryOutputArchive org.apache.jute.CsvOutputArchive org.apache.jute.XmlOutputArchive
* org.apache.jute.Index 用于迭代数据进行反序列化的迭代器接口
* org.apache.jute.Record 在zookeeper 要进行网络通信的对象，都要实现这个接口

#### 持久化机制

zookeeper 是一个leader，follower 对等架构，在内部选举leader，集群上每个节点都保存了整个系统的所有数据，并且数据在磁盘和内存上个有一份。

持久化的一些操作接口在org.apache.zookeeper.server.persistence 包中

#### 网络通信框架

* org.apache.zookeeper.server.Stats 表示ServerCnxn 上的统计数据
* org.apache.zookeeper.Watcher 事件处理接口，事件处理程序需要实现的接口
* org.apache.zookeeper.server.ServerCnxn 服务器连接，表示一个从客户端到服务器的连接
* org.apache.zookeeper.server.NettyServerCnxn 基于netty 的连接的具体实现
* org.apache.zookeeper.server.NIOServerCnxn 基于NIO 的连接的具体实现

#### 监听机制

![](https://oscimg.oschina.net/oscnet/up-e7578710b0a698b966c9299ba7be13ee8f9.png)

* Watcher 接口，定义了process 方法，事件处理接口需要实现的接口
* Event 接口，Watcher 的内部类，无方法
* KeeperState 枚举类型，Event 的内部类，表示zookeeper 所处的状态
* EventType 枚举类型，Event 的内部类，表示zookeeper 中发生的事件类型
* WatchedEvent 表示对zookeeper 上发生变化后的反馈，包含了KeeperState 和EventType
* ClientWatchManager 接口类型，表示客户端的Watch 管理者，其定义了materialized 方法，需子类来实现
* ZKWatchManager Zookeeper 的内部类，继承ClientWatchManager
* MyWatcher ZookeeperMain 的内部类，继承Watcher
* ServerCnxn 接口类型，继承Watcher，表示客户端与服务端的一个连接
* WatchManager 管理Watcher

#### Leader 选举

```
zookeeper 的选举是通过类 Election 来实现的

class FastLeaderElection implements Election {
    class Notification      通知，其他节点返回的投票结果
    class ToSend            发起选举者 发送给其他节点的 投票提议
    class Messager {
        class WorkerSender      发起选举者 提供的一个专门用来发送 ToSend 的线程
        class WorkerReceiver    发起选举者 提供的一个专门用来接收 Notification 的线程
    }
    lookForLeader()     具体的选举方法
}
```

##### Leader 选举概述

Leader 选举是保证分布式数据一致性的关键所在。
当Zookeeper 集群中 的一台服务器出现以下两种情况之一时，需要进入Leader 选举：
* 服务器初始化启动
* 服务器运行期间无法和Leader 保持连接

zookeeper 有三种角色
* leader
* follower
* observer

每一个zookeeper 的守护进程：QuorumPeerMain 有四种工作状态：
* looking
* leading
* following
* observing

QuorumPeerMain 进程启动的时候，都是looking 状态。
会马上发送请求去联系leader，但不一定可以联系到。如果没有找到leader，就需要发起选举，来选举一个leader 出来。
如果zk 集群运行了一段时间，然后发现leader 宕机了，所有饿的follower 接收不到leader 的心跳，就会发起提议来选取leader

##### 重要概念

这三个参数，是用来帮助每个节点更新选票的
* server.id：zk集群中的所有服务器都有的一个全局唯一编号
* zxid：zk内存在进行任何一次事务操作的时候，都会给每一个事务操作生成一个全局唯一的事务编号
* epoch：leader 任期，zk当中，如果换一个leader，就会更新epoch

##### 服务器启动时的leader 选举
若进行leader 选举，则至少需要两台机器，一般组成服务器集群是奇数台服务器。
在集群初始化阶段，当server1 启动时，其单独无法进行和完成leader 选举，当server2 启动时，两台服务器可以进行通信，选举leader 过程如下：

* 每个server 发出一个投票
```
初始情况下，server1 和server2 都会将自己作为leader 服务器来进行投票，每次投票都会包含所推举的服务器myid 和zxid，使用(myid, zxid) 来表示，
此时server1 的投票为(1, 0)，server2 的投票为(2, 0)，然后各自将这个投票发给集群中其他机器
```
* 每个server 接收来自各个服务器的投票
```
集群中的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自looking 状态的服务器等
```
* 处理投票
```
针对每一个投票，服务器都需要将别人的投票和自己的投票进行pk，规则如下：
1、优先检查zxid，zxid 较大的服务器优先作为leader
2、如果zxid 相同，那么就比较myid，myid 较大的服务器作为leader 服务器

对于server1 来说，投票是(1, 0)，接收server2 的投票信息是(2, 0)，
首先比较zxid，均为0，再比较myid，server2 的myid 大，于是更新自己的投票为(2, 0)，然后重新投票
对于server2 而言，其无须更新自己的投票，只是再次向集群中所有集群发出上一次投票信息即可
```
* 统计投票
```
每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受相同的投票信息，
对于server1、server2，统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便已选出了leader
```
* 改变服务器状态
```
一旦确定了leader，每个服务器就会更新自己的状态，如果是follower，那么就变更为following；如果是leader，就变更为leading
```

##### 服务器运行时期的leader 选举
zookeeper 运行期间，leader 与非leader 服务器各司其职，当有非leader 服务器宕机或新加入，此时不影响leader，一旦leader 挂了，那么整个集群将暂停对外服务，重新选举leader。
假设正在运行的有server1、server2、server3，当前leader 是server2，若某一时刻leader 挂了，此时便开始leader 选举：

* 变更状态
```
leader 挂了后，余下的非observer 服务器都会将自己的服务器状态变更为looking，然后开始leader 选举过程
```
* 每个server 会发出一个投票
```
运行期间，每个服务器上的zxid 可能不同，假设server1 的zxid 为13，server3 的zxid 为12
在第一轮投票中，server1 投票(1, 13)，server3 投票(3, 12)，然后将投票发送给集群中的所有机器
```
* 接收来自各个服务器的投票
```
集群的每个服务器收到投票信息后，首先判断投票的有效性，如检查是否是本轮投票、是否来自looking 状态的服务器等。
```
* 处理投票
```
针对每一个投票，服务器都需要将别人的投票和自己的投票进行pk，规则如下：
1、先校验epoch，如果不是最大的epoch，那么这些epoch 将会被全部丢弃
2、再检查zxid，zxid 较大的服务器优先作为leader
3、如果zxid 相同，那么就比较myid，myid 较大的服务器作为leader 服务器

对于server3，投票是(3, 12)，接收server1 的投票是(1, 13)，zxid 较大，那么更新自己的投票为(1, 13)，然后重新投票
对于server1，无须更新自己的投票
```
* 统计投票
```
每次投票后，服务器会统计投票信息，此时有两台服务器接收了(1, 13)，那么已经选出了leader
```
* 改变服务器的状态
```
一旦确定了leader，每个服务器就会更新自己的状态，如果是follower，那么就变更为following；如果是leader，就变更为leading
```













