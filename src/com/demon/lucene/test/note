# Lucene

Lucene 是一个提供搜索的类库，它专注于文本索引和搜索功能，并不是一个完整的搜索程序，搜索程序的其他模块（例如网页抓取、文档处理、服务器运行、用户界面等）需额外实现。

Lucene 能够把从文本解析处理的数据进行索引和搜索，并不关心数据来源、格式，只要数据能转换为文本格式即可。

Lucene 可以索引和搜索存储在文件中的如下数据（包括但不限于）：
* 远程web 服务器上的网页
* 本地文件系统上的文档
* 简单的文本文件
* word 文档
* xml 文档
* html 文档
* pdf 文档

一个简单的搜索引擎结构：
![](https://oscimg.oschina.net/oscnet/dd407fcd8eca9232eec875ca7e38fccb113.jpg)

从web 或本地获取数据，将数据转为文档，然后进行文本提取，在通过索引程序生成索引，存入索引库，当用户输入关键字搜索时，从索引库中找到关键字对应的索引并显示对应的文档数据


# 分词器 Analyzer

Analyzer 是一个抽象类，切分词的具体规则由子类实现。内部通过TokenStream 实现，Tonkenizer 类和TokenFilter 类是TokenStream 的两个子类。
* Tokenizer 处理单个字符组成的字符流，读取Reader 对象中的数据，处理后转换成词汇单元
* TokenFilter完成文本过滤器的功能，但在使用过程中必须注意不同过滤器的使用顺序

#### Lucene 提供的分词器

* StopAnalyzer： 停用词分词器，能过滤词汇中特定的字符串和词汇，并且完成大写转小写的功能
* StandardAnalyzer： 标准分词器，根据空格和符号来完成分词，还可以完成数字、字母、email 地址、IP地址以及中文字符的分析处理，还可以支持过滤词表
* WhitespaceAnalyzer： 空格分词器，使用空格作为间隔符的词汇分词器
* SimpleAnalyzer： 简单分词器，基于西方文字符词汇分析的分词器，处理词汇单元时，以非字母字符作为分割符号。输出的词汇单元完成小写字符转换，去掉标点符号等分割符
* CJKAnalyzer： 二分法分词器，内部调用CJKTokenizer 分词器，对中文进行分词，同时使用StopFilter 过滤器完成过滤功能，可以实现中文的多元切分和停用词过滤
* KeywordAnalyzer： 关键词分词器，把整个输入作为一个单独词汇单元，方便特殊类型的文本进行索引。针对邮政编码、地址等文本信息使用关键词分词器进行索引建立非常方便



